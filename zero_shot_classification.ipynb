{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691ec497",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a890c539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g.fidone/hlt/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# general\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# data\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# custom\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b1ed72",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d63197e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A = load_dataset(\"json\", data_files='subtaskA_test.jsonl', split='train')\n",
    "test_B = load_dataset(\"json\", data_files='subtaskB_test.jsonl', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a215016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'model', 'source', 'id'],\n",
       "    num_rows: 11976\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd41d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'model', 'source', 'label', 'id'],\n",
       "    num_rows: 7103\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f3feb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '  First passage of stochastic processes under resetting has recently been an\\nactive research topic in the field of statistical physics. However, most of\\nprevious studies mainly focused on the systems with continuous time and space.\\nIn this paper, we study the effect of stochastic resetting on first passage\\nproperties of discrete-time absorbing Markov chains, described by a transition\\nmatrix $\\\\brm{Q}$ between transient states and a transition matrix $\\\\brm{R}$\\nfrom transient states to absorbing states. Using a renewal approach, we exactly\\nderive the unconditional mean first passage time (MFPT) to either of absorbing\\nstates, the splitting probability the and conditional MFPT to each absorbing\\nstate. All the quantities can be expressed in terms of a deformed fundamental\\nmatrix $\\\\brm{Z_{\\\\gamma}}=\\\\left[\\\\brm{I}-(1-\\\\gamma) \\\\brm{Q} \\\\right]^{-1}$ and\\n$\\\\brm{R}$, where $\\\\brm{I}$ is the identity matrix, and $\\\\gamma$ is the\\nresetting probability at each time step. We further show a sufficient condition\\nunder which the unconditional MPFT can be optimized by stochastic resetting.\\nFinally, we apply our results to two concrete examples: symmetric random walks\\non one-dimensional lattices with absorbing boundaries and voter model on\\ncomplete graphs.\\n',\n",
       " 'label': 0,\n",
       " 'model': 'human',\n",
       " 'source': 'arxiv',\n",
       " 'id': 107038}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_A[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df136640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Athletics at the 2006 Commonwealth Games â€“ Men\\'s 200 metres Commonwealth Games was held in Delhi, India between 3 and 14 October 2006. The 200 metres event was held on the first day of the Games. Indian sprinter P.V. Sindhu won the most coveted gold medal of the Games. Two other Indian athletes, Sajjad Hiqal and Navendu Khatik, also won medals (silver and bronze) that day. Sindhu\\'s win established herself as the highest ranked Indian athlete in the Women\\'s 200 meters category as of July 2014. The Indian men fielded a team of six athletes for this event. The team, led by Suresh S myo, included Sai Suresh Reddy, Surya Sai Saiyan Reddy, B S Sailo, C.K. Nayudu, and P.R. Sahoo. Indian athletes finished in seventh, ninth, eleventh, thirteenth and seventeenth place respectively. Suresh Smyo, who was leading the race for the entire duration, came home in ninth place with a timing of 20.32 seconds. Indian medal hopes at the Games ended when Sailo and Sahoo finished outside the medals in eleventh and thirteenth places respectively. C.K. Nayudu, who finished third was the only Indian who finished in medals lane. The performance by the Indian team was considered to be the highlight of the Games, with the BBC labeling it as \"One of the proudest moments for India\"',\n",
       " 'model': 'dolly',\n",
       " 'source': 'wikipedia',\n",
       " 'label': 5,\n",
       " 'id': 56748}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_B[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5711f84-b923-493f-b145-7ef2c078662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data, tokenizer, init_prompt, max_length, labeled=False):\n",
    "    '''Variation of custom function tokenize() (see utils.py) which also includes init_prompt.'''\n",
    "\n",
    "    batch_size = len(data['text'])\n",
    "    init_prompt = init_prompt\n",
    "    inputs = [f'{init_prompt}. Text: \"{text}' for text in data['text']] # input text to be passed to the LM\n",
    "    if labeled:\n",
    "      labels = [f'\\nLabel: {label}' for label in data['label']]\n",
    "    else:\n",
    "      labels = [f'\\nLabel: ' for label in data['label']]\n",
    "    tokenized_inputs = tokenizer(inputs) # tokenized input text\n",
    "    tokenized_labels = tokenizer(labels) # tokenized labels\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = tokenized_inputs['input_ids'][i] # input ids of i-th sample\n",
    "        label_input_ids = tokenized_labels['input_ids'][i] # label ids of i-th sample\n",
    "\n",
    "        tokenized_inputs['input_ids'][i] = sample_input_ids + [tokenizer.pad_token_id] * (max_length - len(sample_input_ids)) # right padding\n",
    "        tokenized_inputs['input_ids'][i] = tokenized_inputs['input_ids'][i][:max_length - len(label_input_ids) - 1] + [tokenizer('\"')['input_ids'][1]] # truncation\n",
    "        tokenized_inputs['input_ids'][i] = tokenized_inputs['input_ids'][i] + label_input_ids # adding label\n",
    "        \n",
    "        tokenized_labels['input_ids'][i] = [-100] * (max_length - len(label_input_ids)) + label_input_ids\n",
    "        tokenized_inputs['labels'] = tokenized_labels['input_ids']\n",
    "\n",
    "        tokenized_inputs['attention_mask'][i] = [1] * max_length\n",
    "  \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c5c903-5ba3-4a3d-ae4b-3d9f652d7e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tokenizer_path = 'mistralai/Mistral-7B-v0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d610ac1-b83d-4c42-8f91-d67c760fb508",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_tokenizer_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdce2702-fe6b-4e15-a0d4-5573158f78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 300\n",
    "init_prompt_A = 'Decide if the following text has been written by a human or by a language model. Write 0 if it has been written by a human. Write 1 if it has been written by a language model'\n",
    "init_prompt_B = 'Decide if the text has been written by a human or by a language model among: ChatGPT, Cohere, Davinci, Bloomz or Dolly. Write 0 if it has been written by a human. Write 1 if it has been written by ChatGPT. Write 2 if it has been written by Cohere. Write 3 if it has been written by Davinci. Write 4 if it has been written by Bloomz. Write 5 if it has been written by Dolly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfdb6351-704f-40b0-b28d-c40d7848ef48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf5764555704e22887a688292b7f3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/11976 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_A = test_A.map(\n",
    "    lambda example: tokenize(example, tokenizer, init_prompt_A, max_length, labeled=False),\n",
    "    batched=True,\n",
    "    desc=\"Tokenizing dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f6e830a-ad3f-4f7f-a620-53112f503cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afe113038bc48a1a35cebe62f3f8f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset:   0%|          | 0/7103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_B = test_B.map(\n",
    "    lambda example: tokenize(example, tokenizer, init_prompt_B, max_length, labeled=False),\n",
    "    batched=True,\n",
    "    desc=\"Tokenizing dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aecb4b8-b8a1-49ff-bfbe-23f909e3832f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Decide if the following text has been written by a human (0) or by a language model (1). Text: \"  First passage of stochastic processes under resetting has recently been an\\nactive research topic in the field of statistical physics. However, most of\\nprevious studies mainly focused on the systems with continuous time and space.\\nIn this paper, we study the effect of stochastic resetting on first passage\\nproperties of discrete-time absorbing Markov chains, described by a transition\\nmatrix $\\\\brm{Q}$ between transient states and a transition matrix $\\\\brm{R}$\\nfrom transient states to absorbing states. Using a renewal approach, we exactly\\nderive the unconditional mean first passage time (MFPT) to either of absorbing\\nstates, the splitting probability the and conditional MFPT to each absorbing\\nstate. All the quantities can be expressed in terms of a deformed fundamental\\nmatrix $\\\\brm{Z_{\\\\gamma}}=\\\\left[\\\\brm{I}-(1-\\\\gamma) \\\\brm{Q} \\\\right]^{-1}$ and\\n$\\\\brm{R}$, where $\\\\brm{I}$ is the identity matrix, and $\\\\gamma$ is the\\nresetting probability at each time step. We further show a sufficient condition\\nunder which the unconditional MPFT can be optimized by stochastic resetting. \"<s> \\nLabel: '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(test_A[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "071679e9-cf97-44bf-b8c0-6f3239970fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Decide if the text has been written by a human (0) or by a language model among: ChatGPT (1), Cohere(2), Davinci (3), Bloomz (4) or Dolly (5). Text: \"Athletics at the 2006 Commonwealth Games â€“ Men\\'s 200 metres Commonwealth Games was held in Delhi, India between 3 and 14 October 2006. The 200 metres event was held on the first day of the Games. Indian sprinter P.V. Sindhu won the most coveted gold medal of the Games. Two other Indian athletes, Sajjad Hiqal and Navendu Khatik, also won medals (silver and bronze) that day. Sindhu\\'s win established herself as the highest ranked Indian athlete in the Women\\'s 200 meters category as of July 2014. The Indian men fielded a team of six athletes for this event. The team, led by Suresh S myo, included Sai Suresh Reddy, Surya Sai Saiyan Reddy, B S Sailo, C.K. Nayudu, and P.R. Sahoo. Indian athletes finished in seventh, ninth, eleventh, thirteenth and seventeenth place respectively \"<s> \\nLabel: '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(test_B[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7d51326-05f8-4e03-9b27-9d74d9d2b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_A = test_A['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83f8e868-1076-4640-9c1c-3827bf95c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_B = test_B['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91d9132c-bf6e-4d8b-a792-7df6de18b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A = test_A.remove_columns(['id', 'text', 'source', 'label', 'model'])\n",
    "test_B = test_B.remove_columns(['id', 'text', 'source', 'label', 'model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a12f0d77-2072-460c-a76e-15e9fad272b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A.set_format('pt', columns=['input_ids', 'attention_mask', 'labels'], output_all_columns=True)\n",
    "test_B.set_format('pt', columns=['input_ids', 'attention_mask', 'labels'], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bd75ffe-ba80-4678-a3c7-f88d8b76451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdd25fe2-5e7b-49ff-bdd2-1fc1900ba87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_A = DataLoader(\n",
    "    test_A, \n",
    "    shuffle=False, \n",
    "    collate_fn=default_data_collator, \n",
    "    batch_size=batch_size, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae6bad2b-696c-45ed-8ec0-f49690bfa175",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_B = DataLoader(\n",
    "    test_B, \n",
    "    shuffle=False, \n",
    "    collate_fn=default_data_collator, \n",
    "    batch_size=batch_size, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca59ec-02eb-4185-81d1-045f709222ff",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df8989ed-3f05-4d57-b1fd-ac68595c853f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g.fidone/hlt/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/g.fidone/hlt/lib/python3.10/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/g.fidone/hlt/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f85cc77223b428e94431c1659aa0a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_tokenizer_path, torch_dtype=torch.float16) # loading model in half-precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e49b119-853c-4a38-b9e8-c4eb586bd84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cec8a4ad-2afd-4bc5-8967-6f62c0fd342e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Tesla V100S-PCIE-32GB memory usage: 13.989/31.739 GiB\n"
     ]
    }
   ],
   "source": [
    "check_cuda_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f423dc-5dce-47ef-be5c-27f4ce113f84",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac315ac6-c2a0-40e9-a0d4-451d61942257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset):\n",
    "    outputs = list()\n",
    "    for batch in tqdm(dataset):\n",
    "      batch = {k : v.to(device) for k, v in batch.items()} # moving batches to GPU\n",
    "      batch_outputs = model.generate(**batch, max_new_tokens=1, pad_token_id=tokenizer.eos_token_id) # auto-regressive generation imposing max_new_tokens=1\n",
    "      decoded_batch_outputs = tokenizer.batch_decode(batch_outputs.detach().cpu().numpy(), skip_special_tokens=True)\n",
    "      outputs.extend(decoded_batch_outputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5d95541-0b14-483a-9565-bb9c99c5eb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d0c025d9d14961831486955f765a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs_A = inference(test_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c872afc9-7361-4667-b10c-49ff7a14f47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95ae94a945d4defbb1219584a1c9c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/888 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs_B = inference(test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aece037f-4d0a-4ed5-bcf4-0f5b94cae1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g.fidone/utils.py:130: UserWarning: A number of 1817 labels have been randomly selected because generated labels did not match any expected label.\n",
      "  warnings.warn(f'A number of {missed} labels have been randomly selected because generated labels did not match any expected label.', UserWarning)\n",
      "/home/g.fidone/utils.py:130: UserWarning: A number of 641 labels have been randomly selected because generated labels did not match any expected label.\n",
      "  warnings.warn(f'A number of {missed} labels have been randomly selected because generated labels did not match any expected label.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "y_pred_A = get_labels_from_texts(outputs_A, [0, 1]) # custom function (see utils.py)\n",
    "y_pred_B = get_labels_from_texts(outputs_B, [0, 1, 2, 3, 4, 5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0dc5819-7f5b-4756-b2c6-de5090763da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.518     0.871     0.650      6298\n",
      "           1      0.418     0.103     0.165      5678\n",
      "\n",
      "    accuracy                          0.507     11976\n",
      "   macro avg      0.468     0.487     0.407     11976\n",
      "weighted avg      0.471     0.507     0.420     11976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_A, y_pred_A, digits=3)) # task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "912bce77-1969-4dc7-b093-edc4308c5d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.159     0.899     0.271      1198\n",
      "           1      0.103     0.008     0.015      1148\n",
      "           2      0.183     0.011     0.021      1155\n",
      "           3      0.213     0.011     0.021      1189\n",
      "           4      0.175     0.009     0.017      1242\n",
      "           5      0.000     0.000     0.000      1171\n",
      "\n",
      "    accuracy                          0.158      7103\n",
      "   macro avg      0.139     0.156     0.057      7103\n",
      "weighted avg      0.140     0.158     0.058      7103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_B, y_pred_B, digits=3)) # task B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt",
   "language": "python",
   "name": "hlt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
